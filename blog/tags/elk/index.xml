<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Elk on Jordan Wright</title>
    <link>http://jordan-wright.com/blog/tags/elk/</link>
    <description>Recent content in Elk on Jordan Wright</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jun 2016 06:30:00 -0500</lastBuildDate>
    <atom:link href="http://jordan-wright.com/blog/tags/elk/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Analyzing 5 Years of Police Call Data</title>
      <link>http://jordan-wright.com/blog/post/2016-05-06-exploring-sapd-call-data-with-elk/</link>
      <pubDate>Wed, 22 Jun 2016 06:30:00 -0500</pubDate>
      
      <guid>http://jordan-wright.com/blog/post/2016-05-06-exploring-sapd-call-data-with-elk/</guid>
      <description>

&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/headers/sapd.png&#34; alt=&#34;&#34; class=&#34;pure-img&#34; &gt;


&lt;p&gt;San Antonio is a great city. &lt;a href=&#34;http://www.yelp.com/search?cflt=mexican&amp;amp;find_loc=San+Antonio%2C+TX%2C+USA&#34;&gt;According to Yelp&lt;/a&gt;, there are over 1200 places to get a taco - how could it &lt;em&gt;not&lt;/em&gt; be great?&lt;/p&gt;

&lt;p&gt;Unfortunately, any time you get a huge group of people together there will be crime, and SA is no exception. Our SAPD stay busy &lt;sup&gt;24&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt;, constantly putting their lives on the line to keep the city safe, and I&amp;rsquo;m thankful for all the work they do.&lt;/p&gt;

&lt;p&gt;Being an amateur API aficionado, I was excited to find the &lt;a href=&#34;http://www.sanantonio.gov/SAPD/SAPDOpenDataInitiative.aspx#182281929-open-data&#34;&gt;SAPD Open Data Initiative&lt;/a&gt; that contains a wealth of information on the activities the SAPD perform. Specifically, I wanted to see what kinds of analytics I could gather from exploring the historic SAPD call data.&lt;/p&gt;

&lt;p&gt;In this post, I&amp;rsquo;ll explain how I was able to gather and analyze &lt;strong&gt;4.3 million call data records&lt;/strong&gt;, or how I basically became the extremely boring part of Batman.&lt;/p&gt;

&lt;h3 id=&#34;getting-the-call-data&#34;&gt;Getting the Call Data&lt;/h3&gt;

&lt;h4 id=&#34;a-little-about-the-format&#34;&gt;A Little About the Format&lt;/h4&gt;

&lt;p&gt;The first thing we have to do is get the call data. As part of the Open Data Initiative, SAPD &lt;a href=&#34;http://www.sanantonio.gov/SAPD/Calls.aspx&#34;&gt;publishes historical calls&lt;/a&gt; on their website. This interface lets you search through calls that were responded to by an SAPD officer dating back to January 1, 2011.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/sapd/call_details.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Each call record has the following details:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Incident Number&lt;/strong&gt; - A unique identifier for the call in SAPD-YYYY-xxxxxxx format&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Category&lt;/strong&gt; - The category of the call (e.g. &amp;ldquo;Crimes Against Person Calls&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problem Type&lt;/strong&gt; - The sub-type of the call that narrows down from the root category (e.g. &amp;ldquo;Robbery of Individual&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Response Date&lt;/strong&gt; - The date/time a response was given to the call&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Address&lt;/strong&gt; - The address where the incident occurred&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HOA&lt;/strong&gt; - The Homeowner&amp;rsquo;s Association where the incident occurred (if applicable)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;School District&lt;/strong&gt; - The school district where the incident occurred&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Council District&lt;/strong&gt; - The council district where the incident occurred&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a &lt;em&gt;ton&lt;/em&gt; of great data. However, searching through the data manually is a pain, and I didn&amp;rsquo;t see any obvious bulk export feature. Time to get out the whiskey and BeautifulSoup and get to scraping.&lt;/p&gt;

&lt;h4 id=&#34;scraping-the-data&#34;&gt;Scraping the Data&lt;/h4&gt;

&lt;p&gt;Scraping the site turned out to be a chore for multiple reasons, the first of which was due to the &lt;a href=&#34;https://www.ssllabs.com/ssltest/analyze.html?d=webapps2.sanantonio.gov&amp;amp;hideResults=on&#34;&gt;SSL configuration in use&lt;/a&gt;. I wound up having to use a custom TLS adapter just to get the &lt;code&gt;requests&lt;/code&gt; library to negotiate a TLS connection.&lt;/p&gt;

&lt;p&gt;Then I had to address the restrictions on how I could search. I had to filter by a certain category and I could only get at most 10k results per query. To put things in perspective, that lets me &lt;em&gt;usually&lt;/em&gt; get a week of calls labelled as &amp;ldquo;Other&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No one can say our officers aren&amp;rsquo;t busy.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I ended up hacking together a scraper that ran like this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For every category

&lt;ul&gt;
&lt;li&gt;Try to get a week&amp;rsquo;s worth of calls&lt;/li&gt;
&lt;li&gt;If # calls = 10000

&lt;ul&gt;
&lt;li&gt;Get that week a day at a time to make sure we don&amp;rsquo;t miss anything&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Store the results&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://gist.github.com/jordan-wright/92e209d89a4174e3ccb50ed4f909e58e&#34;&gt;Here&amp;rsquo;s&lt;/a&gt; the current code - be warned, it&amp;rsquo;s hacky. I was manually changing the categories to make sure things were going smoothly, so YMMV.&lt;/p&gt;

&lt;p&gt;After finishing up the scraping, I was left with &lt;strong&gt;4.3 million records&lt;/strong&gt; to work with.&lt;/p&gt;

&lt;h3 id=&#34;analyzing-the-data&#34;&gt;Analyzing the Data&lt;/h3&gt;

&lt;p&gt;After grabbing all the data, I wrote a quick script to dump it into ELK. Loading up Kibana, we see our sweet, sweet data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/sapd/calls_over_time.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;We can immediately see that calls are surprisingly consistent in terms of frequency. Digging into the data, we can find some other neat insights:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theft and Crimes Against People&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I started filtering the data looking for theft/burglary and crimes against people. Looking at the top 10 call types, the generic call description of &amp;ldquo;theft&amp;rdquo; was consistently the highest:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/sapd/person_and_theft.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;With this data, it was easy to track down the most dangerous zip code and address by filtering to just the &amp;ldquo;crimes against persons&amp;rdquo; category. With right around 40k calls, zipcode 78207 (mapped below) accounts for 7.1% of all crimes against people:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/sapd/zipcodes_person.png&#34;/&gt;&lt;/p&gt;

&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/sapd/78207.png&#34; alt=&#34;&#34; class=&#34;pure-img&#34; &gt;


&lt;p&gt;We can do the same filtering to find the most dangerous school district - San Antonio ISD comes in the highest at 196k calls, which is around 36%:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/sapd/school_crimes.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When Does Crime Happen?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This was an interesting one, since I couldn&amp;rsquo;t find a good way to display this in Kibana. To Python, more whiskey, and Excel we go. I wrote a quick script to parse the &amp;ldquo;Crimes Against Persons&amp;rdquo; calls to find the distribution by minute. Obviously, this will depend on where you live, what time of the year it is, etc. but here is the graph:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/sapd/crimes_by_minute.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Traffic Related Calls&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The calls labelled as &amp;ldquo;traffic related&amp;rdquo; deal with traffic stops, DWIs, etc. I did the same analysis as before to find out when most traffic related calls happen and got these results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/sapd/traffic_calls_by_minute.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Looking at the top addresses for traffic stops and DWI&amp;rsquo;s, we see that downtown is the place where most incidents happen:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/sapd/traffic_locations.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other Calls&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;other&amp;rdquo; category gives some interesting datapoints - most of these deal with various disturbances. For example, we can see that the number of &amp;ldquo;fireworks disturbance&amp;rdquo; calls always spikes around July 4th, New Year&amp;rsquo;s Eve, and my birthday &lt;sup&gt;not really&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/sapd/fireworks.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Issues with Geocoding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It was my original intent to geocode the records so that I could plot them in a heatmap fashion. Unfortunately, I ran into a couple of challenges:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Finding a geocoder&lt;/strong&gt; - Even though there are only about 233k unique addresses found in the data, I still ran into issues finding a free service to geocode that many addresses. I started working with Mapzen (which is pretty awesome), when I discovered my other problem:&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Generic Addresses&lt;/strong&gt; - Many of the addresses in the dataset are &lt;em&gt;incredibly&lt;/em&gt; vague. For example, the address with the most number of &amp;ldquo;traffic related&amp;rdquo; calls? &amp;ldquo;IH 10 W&amp;rdquo;, followed by &amp;ldquo;NW Loop 410&amp;rdquo;. Go figure.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both of these made it difficult to come up with meaningful heatmaps to show crime over the SA region. If anyone has any ideas, hit me up.&lt;/p&gt;

&lt;h3 id=&#34;sweet-sweet-dashboards&#34;&gt;Sweet, Sweet Dashboards&lt;/h3&gt;

&lt;p&gt;One of the best parts about the ELK stack is the ability to create dashboards like this one:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/sapd/dashboard.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;This dashboard gives a great overview of the call details, with the ability to drill down into the calls you care about.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t think it&amp;rsquo;s any stretch to say that having this visibility into crime across the city &lt;strong&gt;basically makes me&lt;/strong&gt; &lt;sup&gt;the boring part of&lt;/sup&gt; &lt;strong&gt;Batman&lt;/strong&gt;. After all, have you ever seen Batman and me in the same room?&lt;/p&gt;

&lt;p&gt;Didn&amp;rsquo;t think so.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/sapd/batman.png&#34; style=&#34;max-height:400px;&#34; alt=&#34;&#34;&gt;

&lt;br&gt;&lt;/p&gt;

&lt;h3 id=&#34;have-some-data&#34;&gt;Have Some Data&lt;/h3&gt;

&lt;p&gt;Since the purpose of this open data initiative was to share information with the community, I want to do my part by making the dataset available as JSON so that people who are much better than me at analyzing large data sets can find new insights that will help make SA a safer place to live.&lt;/p&gt;

&lt;p&gt;The dataset contains all records from January 1, 2011 to November 30, 2015, which was when I pulled the data.&lt;/p&gt;

&lt;p&gt;You can download the full dataset &lt;a href=&#34;https://drive.google.com/open?id=0B9Ile8onstUBbWgxakprZk9JYjA&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s help our officers make this city safer.&lt;/p&gt;

&lt;p&gt;-Jordan (&lt;a href=&#34;https://twitter.com/jw_sec&#34;&gt;@jw_sec&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2 Years of @dumpmon</title>
      <link>http://jordan-wright.com/blog/2015/05/26/two-years-of-at-dumpmon/</link>
      <pubDate>Tue, 26 May 2015 00:00:00 +0000</pubDate>
      
      <guid>http://jordan-wright.com/blog/2015/05/26/two-years-of-at-dumpmon/</guid>
      <description>

&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/headers/dumpmon_header.png&#34; alt=&#34;&#34; class=&#34;pure-img&#34; &gt;


&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;This post is long overdue.&lt;/p&gt;

&lt;p&gt;Back in May 2013, I &lt;a href=&#34;http://raidersec.blogspot.com/2013/03/introducing-dumpmon-twitter-bot-that.html&#34;&gt;released&lt;/a&gt; a Twitter bot called &lt;a href=&#34;http://twitter.com/dumpmon&#34;&gt;@dumpmon&lt;/a&gt; whose sole purpose was to track and report password dumps and other sensitive information shared on paste sites such as Pastebin. Since that time, dumpmon has proven - to my excitement - to be valuable to researchers, being featured in &lt;a href=&#34;http://arstechnica.com/security/2013/06/raspberry-pi-bot-tracks-hacker-posts-to-vacuum-up-passwords-and-more/&#34;&gt;news articles&lt;/a&gt;, Defcon slides, and &lt;a href=&#34;http://haveibeenpwned.com&#34;&gt;HIBP&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;After two years, it&amp;rsquo;s time to post an overdue status update providing some insight into the data dumpmon has collected over this time.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: This is a pretty long post, so feel free to skip &lt;a href=&#34;#data&#34;&gt;here&lt;/a&gt; if you just want the data.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;collecting-the-data-mongo-elk&#34;&gt;Collecting the Data: Mongo + ELK&lt;/h3&gt;

&lt;p&gt;Soon after dumpmon went live, I noticed that sensitive pastes were getting deleting shortly after being posted. This makes it difficult to use the data for any long term research, so I started keeping copies.&lt;/p&gt;

&lt;p&gt;First, I used MongoDB to store each paste and meta information (such as &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;num_emails&lt;/code&gt;, &lt;code&gt;num_hashes&lt;/code&gt;, etc.). This worked well for about a year, but it put quite a bit of strain on my Raspberry Pi. Additionally, it was cumbersome to query the data and visualize the statistics over time.&lt;/p&gt;

&lt;p&gt;I decided to move everything over to ELK. Unfortunately, in the process of doing so, the DB got corrupted causing most of the data to be lost. &lt;em&gt;Not a good day.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;However, I eventually got everything up and running with copies of the data going to both MongoDB (just in case), as well as ELK. Here&amp;rsquo;s a partial screenshot of the dashboard:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/dumpmon_report/dashboard.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Having this dashboard in ELK lets me see trends over time, search for individual paste content, as well as track pastes related to a campaign over time.&lt;/p&gt;

&lt;h3 id=&#34;analyzing-the-data&#34;&gt;Analyzing the Data&lt;/h3&gt;

&lt;p&gt;Enough history - let&amp;rsquo;s take a look at the data!&lt;/p&gt;

&lt;p&gt;As mentioned before, most of the older data was lost in the process of migrating to ELK, but we still have data dating back to June 2014 to analyze.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s first talk about the overall kinds of data kinds of data found. Currently, dumpmon looks for database dumps, Cisco config files, Google API keys, and SSH private keys. Overwhemingly, database dumps are the most common type of paste found, coming in at 92%.&lt;/p&gt;

&lt;p&gt;Each of these database dumps can contain a combination of usernames, email addresses, passwords (plain text or hashed), and more.&lt;/p&gt;

&lt;p&gt;After a few weeks of running the bot, is was clear that password dumps occur far more often than I thought. On average, &lt;strong&gt;dumpmon found 46 significant (by its thresholds) database dumps every day.&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&#34;cumulative-results&#34;&gt;Cumulative Results&lt;/h4&gt;

&lt;p&gt;Here are a few more cumulative numbers for those interested:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;32.8k potential database dumps total&lt;/li&gt;
&lt;li&gt;540 emails, on average, per dump (since June)&lt;/li&gt;
&lt;li&gt;5 million unique emails found (since June)&lt;/li&gt;
&lt;li&gt;1 million unique hashes found (since June)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;twitter-account-statistics&#34;&gt;Twitter Account Statistics&lt;/h3&gt;

&lt;p&gt;Thanks to the Twitter Analytics platform, we can get insight into the statistics regarding the actual dumpmon Twitter account.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/dumpmon_report/twitter_followers.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;As you can see above, the number of followers has steadily increased over time. While Twitter won&amp;rsquo;t give me trends for the entire time dumpmon has been running, here is some data for the last 28 days:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jordan-wright.com/blog/blog/images/blog/dumpmon_report/general_stats.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;div id=&#34;data&#34;&gt;&lt;/div&gt;&lt;/p&gt;

&lt;h3 id=&#34;here-have-some-data&#34;&gt;Here, Have Some Data&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m a big fan of giving out raw data. While I have hesitations to release all the raw data dumpmon has found (&lt;a href=&#34;http://jordan-wright.com/blog/contact&#34;&gt;contact&lt;/a&gt; me if you&amp;rsquo;re a researcher who needs this data), I&amp;rsquo;m happy to give out all of the information I have about dumpmon&amp;rsquo;s Twitter activity. This includes dumpmon&amp;rsquo;s entire tweet archive, aggregated analytics data, and more.&lt;/p&gt;

&lt;p&gt;&lt;img style=&#34;vertical-align:middle;&#34; src=&#34;http://jordan-wright.com/blog/blog/images/blog/dumpmon_report/zip.png&#34;/&gt;
&lt;strong&gt;Tweet Archive&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jordan-wright.com/blog/downloads/dumpmon_archive.zip&#34;&gt;This&lt;/a&gt; is dumpmon&amp;rsquo;s exported Twitter archive. It contains &lt;strong&gt;every tweet&lt;/strong&gt; sent out by dumpmon. You should use this data if you want to extract out metrics regarding number of emails per dump, etc.&lt;/p&gt;

&lt;p&gt;Specifically, the &lt;code&gt;tweets.csv&lt;/code&gt; file in this archive contains the following fields:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tweet_id, in_reply_to_status_id, in_reply_to_user_id, timestamp, source, text, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, expanded_urls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://jordan-wright.com/blog/downloads/dumpmon_archive.zip&#34;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img style=&#34;vertical-align:middle;&#34; src=&#34;http://jordan-wright.com/blog/blog/images/blog/dumpmon_report/analytics.png&#34;/&gt;
&lt;strong&gt;Tweet Analytics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jordan-wright.com/blog/downloads/tweet_activity_metrics_dumpmon.csv&#34;&gt;This file&lt;/a&gt; contains the analytics of every tweet after 04/30/2014 (22k tweets). It&amp;rsquo;s in CSV format and has the following fields:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Tweet id, Tweet permalink, Tweet text, time, impressions, engagements, engagement rate, retweets, replies, favorites, user profile clicks, url clicks, hashtag clicks, detail expands, permalink clicks
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://jordan-wright.com/blog/downloads/tweet_activity_metrics_dumpmon.csv&#34;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I hope this post gave a long-overdue update of where dumpmon is today. I recently moved dumpmon to a new home on a Digital Ocean VPS that&amp;rsquo;s bigger, faster, and has more storage than my Pi.&lt;/p&gt;

&lt;p&gt;It should be noted that dumpmon is just getting started. While there haven&amp;rsquo;t been too many changes to the codebase in a while, I&amp;rsquo;ve seen people expanding the codebase in ways I&amp;rsquo;m excited to bring mainline.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;All in all, @dumpmon is here to stay.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As always, please don&amp;rsquo;t hesitate to let me know if you have any questions or comments!&lt;/p&gt;

&lt;p&gt;-Jordan (&lt;a href=&#34;http://twitter.com/jw_sec&#34;&gt;@jw_sec&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>